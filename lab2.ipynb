{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "qf3z5SqWZ91b",
        "g1GznJhObXPk",
        "Ru8vllrMbgvL",
        "z0MWgLingzhw",
        "lrLs_T2Qd0kc",
        "Gr9BxL8zeBfv"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saccuz/AML_lab/blob/main/lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf3z5SqWZ91b"
      },
      "source": [
        "# Torch "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbtEmI1AiTkF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "df3a58b9-5e6e-4173-cae6-604c64c450e8"
      },
      "source": [
        "!pip3 install 'torch==1.3.1'\n",
        "!pip3 install 'torchvision==0.5.0'\n",
        "!pip3 install 'Pillow-SIMD'\n",
        "!pip3 install 'tqdm'\n",
        "!pip3 install \"colorama\"\n",
        "\n",
        "import torch\n",
        "#use GPU if available \n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #'cpu' # 'cuda' or 'cpu'\n",
        "print(DEVICE)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch==1.3.1\n",
            "  Downloading torch-1.3.1-cp37-cp37m-manylinux1_x86_64.whl (734.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 734.6 MB 16 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.3.1) (1.21.6)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.3.1 which is incompatible.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.3.1 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.3.1 which is incompatible.\n",
            "fastai 2.7.10 requires torch<1.14,>=1.7, but you have torch 1.3.1 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchvision==0.5.0\n",
            "  Downloading torchvision-0.5.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 5.4 MB/s \n",
            "\u001b[?25hCollecting torch==1.4.0\n",
            "  Downloading torch-1.4.0-cp37-cp37m-manylinux1_x86_64.whl (753.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 753.4 MB 6.6 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.5.0) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchvision==0.5.0) (1.15.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.5.0) (7.1.2)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.3.1\n",
            "    Uninstalling torch-1.3.1:\n",
            "      Successfully uninstalled torch-1.3.1\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.13.1+cu113\n",
            "    Uninstalling torchvision-0.13.1+cu113:\n",
            "      Successfully uninstalled torchvision-0.13.1+cu113\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.4.0 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.4.0 which is incompatible.\n",
            "fastai 2.7.10 requires torch<1.14,>=1.7, but you have torch 1.4.0 which is incompatible.\n",
            "fastai 2.7.10 requires torchvision>=0.8.2, but you have torchvision 0.5.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.4.0 torchvision-0.5.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Pillow-SIMD\n",
            "  Downloading Pillow-SIMD-9.0.0.post1.tar.gz (849 kB)\n",
            "\u001b[K     |████████████████████████████████| 849 kB 5.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Pillow-SIMD\n",
            "  Building wheel for Pillow-SIMD (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for Pillow-SIMD: filename=Pillow_SIMD-9.0.0.post1-cp37-cp37m-linux_x86_64.whl size=1245586 sha256=a70daca302ecd409a3d7343b65c6a1bb658023cb541c16fce6e48282056e219a\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/3f/fd/ca7133b4f7f509eb1de652bb8c128529a0c04b25ef0a6c535a\n",
            "Successfully built Pillow-SIMD\n",
            "Installing collected packages: Pillow-SIMD\n",
            "Successfully installed Pillow-SIMD-9.0.0.post1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Installing collected packages: colorama\n",
            "Successfully installed colorama-0.4.6\n",
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czjvnq3FjBmh"
      },
      "source": [
        "# Download Dataset GTEA61"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGOhXMNqjMed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5bb136f-cf9d-4f4a-ddde-a1cae656a30f"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "#drive.mount('/content/drive')\n",
        "import sys, os\n",
        "           \n",
        "#1YKfdhB9Xxh4pmND1V3gcm3Gyjc8v8idq\n",
        "if not os.path.isfile('/content/GTEA61.zip'):\n",
        "  !gdown --id 1Z5RWA8yKIy0PvxMlScV-aAz22ITtivfk # 3-5 min\n",
        "  !jar xvf  \"/content/GTEA61.zip\"\n",
        "\n",
        "if not os.path.isdir('/content/GTEA61'):\n",
        "  print(\"Dataset doesn't exist\")\n",
        "\n",
        "#Weights\n",
        "if not os.path.isfile(\"/content/best_model_state_dict_rgb_split2.pth\"):\n",
        "  !gdown --id 1B7Xh6hQ9Py8fmL-pjmLzlCent6dnuex5 # 3-5 min\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Access denied with the following error:\n",
            "\n",
            " \tToo many users have viewed or downloaded this file recently. Please\n",
            "\ttry accessing the file again later. If the file you are trying to\n",
            "\taccess is particularly large or is shared with many people, it may\n",
            "\ttake up to 24 hours to be able to view or download the file. If you\n",
            "\tstill can't access a file after 24 hours, contact your domain\n",
            "\tadministrator. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=1Z5RWA8yKIy0PvxMlScV-aAz22ITtivfk \n",
            "\n",
            "java.io.FileNotFoundException: /content/GTEA61.zip (No such file or directory)\n",
            "\tat java.base/java.io.FileInputStream.open0(Native Method)\n",
            "\tat java.base/java.io.FileInputStream.open(FileInputStream.java:219)\n",
            "\tat java.base/java.io.FileInputStream.<init>(FileInputStream.java:157)\n",
            "\tat java.base/java.io.FileInputStream.<init>(FileInputStream.java:112)\n",
            "\tat jdk.jartool/sun.tools.jar.Main.run(Main.java:407)\n",
            "\tat jdk.jartool/sun.tools.jar.Main.main(Main.java:1680)\n",
            "Dataset doesn't exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kk0rtAlnSlDF"
      },
      "source": [
        "\n",
        "# Download Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8xcWfReaSd_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e98644ad-18a4-49da-85c6-b1498bc9ddba"
      },
      "source": [
        "!git clone \"https://github.com/plana93/Homework_AIML.git\" \n",
        "#!rm -r \"/content/Homework_AIML\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Homework_AIML'...\n",
            "remote: Enumerating objects: 78, done.\u001b[K\n",
            "remote: Total 78 (delta 0), reused 0 (delta 0), pack-reused 78\u001b[K\n",
            "Unpacking objects: 100% (78/78), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiwWzjzSio-h"
      },
      "source": [
        "\n",
        "\n",
        "# Import Code\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wl6fSd3MXofW"
      },
      "source": [
        "import os\n",
        "import logging\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.backends import cudnn\n",
        "import torchvision\n",
        "from colorama import init\n",
        "from colorama import Fore, Back, Style\n",
        "\n",
        "from torchvision.models import resnet34\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"/content/Homework_AIML/\")\n",
        "import Homework_AIML\n",
        "from Homework_AIML import *\n",
        "\n",
        "from gtea_dataset import GTEA61, GTEA61_flow, GTEA61_2Stream\n",
        "from spatial_transforms import (Compose, ToTensor, CenterCrop, Scale, Normalize, MultiScaleCornerCrop,\n",
        "                                RandomHorizontalFlip)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1GznJhObXPk"
      },
      "source": [
        "#**Learning without Temporal information** (avgpool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy4KrHClbAmC"
      },
      "source": [
        "#MAIN PARAMs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-tz9mHPbCYW"
      },
      "source": [
        "homework_step = 0 #--> Learning without Temporal information (avgpool)\n",
        "#homework_step = 1 #--> Learning with Temporal information (LSTM)\n",
        "#homework_step = 2 #--> Learning with Spatio-Temporal information (ConvLSTM)\n",
        "\n",
        "\n",
        "\n",
        "DATA_DIR = '/content/GTEA61/' #path dataset\n",
        "model_folder = '/content/saved_models/' + \"/\" + \"homework_step\"+ str(homework_step) + \"/\" #path to save model \n",
        "if not os.path.isdir(model_folder):\n",
        "    os.makedirs(model_folder)\n",
        "\n",
        "\n",
        "# All this param can be change!\n",
        "\n",
        "NUM_CLASSES = 61     \n",
        "BATCH_SIZE = 64 \n",
        "LR = 0.001            # The initial Learning Rate\n",
        "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = 4e-5  # Regularization, you can keep this at the default\n",
        "NUM_EPOCHS = 200     # Total number of training epochs (iterations over dataset)\n",
        "STEP_SIZE = [25, 75, 150] # How many epochs before decreasing learning rate (if using a step-down policy)\n",
        "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
        "MEM_SIZE = 512       # Dim of internal state of LSTM or ConvLSTM\n",
        "SEQ_LEN = 3          # Num Frames\n",
        "\n",
        "# this dictionary is needed for the logger class\n",
        "parameters = {'DEVICE':DEVICE, 'NUM_CLASSES':NUM_CLASSES, 'BATCH_SIZE':BATCH_SIZE,\n",
        "             'LR':LR, 'MOMENTUM':MOMENTUM, 'WEIGHT_DECAY':WEIGHT_DECAY, 'NUM_EPOCHS':NUM_EPOCHS,\n",
        "             'STEP_SIZE':STEP_SIZE, 'GAMMA':GAMMA, 'MEM_SIZE':MEM_SIZE, 'SEQ_LEN':SEQ_LEN}"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPwkOR8taVdN"
      },
      "source": [
        "#Dataloaders & Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmxnQXSYaeWv"
      },
      "source": [],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LT_Gy79SgBLq"
      },
      "source": [
        "# Normalize\n",
        "normalize = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "spatial_transform = Compose([Scale(256), RandomHorizontalFlip(), MultiScaleCornerCrop([1, 0.875, 0.75, 0.65625], 224),\n",
        "                             ToTensor(), normalize])\n",
        "spatial_transform_val = Compose([Scale(256), CenterCrop(224), ToTensor(), normalize])\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "T69vfGGhjKa_",
        "outputId": "23ca666d-85bc-407b-f8f9-ce10c892d888"
      },
      "source": [
        "# Prepare Pytorch train/test Datasets\n",
        "train_dataset = GTEA61(DATA_DIR, split='train', transform=spatial_transform, seq_len=SEQ_LEN)\n",
        "test_dataset = GTEA61(DATA_DIR, split='test', transform=spatial_transform_val, seq_len=SEQ_LEN)\n",
        "\n",
        "# Check dataset sizes\n",
        "print('Train Dataset: {}'.format(len(train_dataset)))\n",
        "print('Test Dataset: {}'.format(len(test_dataset)))\n",
        "\n",
        "# Dataloaders iterate over pytorch datasets and transparently provide useful functions (e.g. parallelization and shuffling)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, drop_last=True)\n",
        "val_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-0696d9ba587b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Prepare Pytorch train/test Datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGTEA61\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspatial_transform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEQ_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGTEA61\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspatial_transform_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSEQ_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Check dataset sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/Homework_AIML/gtea_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, split, seq_len, transform, target_transform, label_map, mmaps, mmaps_transform, static_frames)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# we expect datadir to be GTEA61, so we add FRAME_FOLDER to get to the frames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mframe_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatadir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFRAME_FOLDER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0musers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0musers_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0musers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/GTEA61/processed_frames2'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21wjiwvW-OPQ"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMWuE-4SHxoY"
      },
      "source": [
        "import torch\n",
        "import resnetMod\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "\n",
        "# LSTM\n",
        "class MyLSTMCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyLSTMCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.i_xx = nn.Linear(input_size, hidden_size)\n",
        "        self.i_hh = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "        self.f_xx = nn.Linear(input_size, hidden_size)\n",
        "        self.f_hh = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "        self.c_xx = nn.Linear(input_size, hidden_size)\n",
        "        self.c_hh = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "\n",
        "        self.o_xx = nn.Linear(input_size, hidden_size)\n",
        "        self.o_hh = nn.Linear(hidden_size, hidden_size, bias=False)\n",
        "        \n",
        "        torch.nn.init.xavier_normal_(self.i_xx.weight)\n",
        "        torch.nn.init.constant_(self.i_xx.bias, 0)\n",
        "        torch.nn.init.xavier_normal_(self.i_hh.weight)\n",
        "\n",
        "        torch.nn.init.xavier_normal_(self.f_xx.weight)\n",
        "        torch.nn.init.constant_(self.f_xx.bias, 0)\n",
        "        torch.nn.init.xavier_normal_(self.f_hh.weight)\n",
        "\n",
        "        torch.nn.init.xavier_normal_(self.c_xx.weight)\n",
        "        torch.nn.init.constant_(self.c_xx.bias, 0)\n",
        "        torch.nn.init.xavier_normal_(self.c_hh.weight)\n",
        "\n",
        "        torch.nn.init.xavier_normal_(self.o_xx.weight)\n",
        "        torch.nn.init.constant_(self.o_xx.bias, 0)\n",
        "        torch.nn.init.xavier_normal_(self.o_hh.weight)\n",
        "        \n",
        "\n",
        "    def forward(self, x, state):\n",
        "        if state is None:\n",
        "            state = (Variable(torch.randn(x.size(0), x.size(1)).cuda()),\n",
        "                     Variable(torch.randn(x.size(0), x.size(1)).cuda()))\n",
        "        \n",
        "        ix = self.i_xx(x)\n",
        "        ih = self.i_hh(state[0])\n",
        "        it = torch.sigmoid(ix+ih)\n",
        "\n",
        "        ft = torch.sigmoid(self.f_xx(x) + self.f_hh(state[0]))\n",
        "        gt = torch.tanh(self.c_xx(x) + self.c_hh(state[0]))\n",
        "        ot = torch.sigmoid(self.o_xx(x) + self.o_hh(state[0]))\n",
        "\n",
        "        ct = ft * state[1]  + it * gt\n",
        "        ht = ot * torch.tanh(ct)\n",
        "\n",
        "        return  ht, ct\n",
        "\n",
        "\n",
        "#ConvLSTM\n",
        "class MyConvLSTMCell(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, kernel_size=3, stride=1, padding=1):\n",
        "        super(MyConvLSTMCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.conv_i_xx = nn.Conv2d(input_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.conv_i_hh = nn.Conv2d(hidden_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                                   bias=False)\n",
        "\n",
        "        self.conv_f_xx = nn.Conv2d(input_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.conv_f_hh = nn.Conv2d(hidden_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                                   bias=False)\n",
        "\n",
        "        self.conv_c_xx = nn.Conv2d(input_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.conv_c_hh = nn.Conv2d(hidden_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                                   bias=False)\n",
        "\n",
        "        self.conv_o_xx = nn.Conv2d(input_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.conv_o_hh = nn.Conv2d(hidden_size, hidden_size, kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                                   bias=False)\n",
        "\n",
        "        torch.nn.init.xavier_normal_(self.conv_i_xx.weight)\n",
        "        torch.nn.init.constant_(self.conv_i_xx.bias, 0)\n",
        "        torch.nn.init.xavier_normal_(self.conv_i_hh.weight)\n",
        "\n",
        "        torch.nn.init.xavier_normal_(self.conv_f_xx.weight)\n",
        "        torch.nn.init.constant_(self.conv_f_xx.bias, 0)\n",
        "        torch.nn.init.xavier_normal_(self.conv_f_hh.weight)\n",
        "\n",
        "        torch.nn.init.xavier_normal_(self.conv_c_xx.weight)\n",
        "        torch.nn.init.constant_(self.conv_c_xx.bias, 0)\n",
        "        torch.nn.init.xavier_normal_(self.conv_c_hh.weight)\n",
        "\n",
        "        torch.nn.init.xavier_normal_(self.conv_o_xx.weight)\n",
        "        torch.nn.init.constant_(self.conv_o_xx.bias, 0)\n",
        "        torch.nn.init.xavier_normal_(self.conv_o_hh.weight)\n",
        "\n",
        "    def forward(self, x, state):\n",
        "        if state is None:\n",
        "            state = (Variable(torch.randn(x.size(0), x.size(1), x.size(2), x.size(3)).cuda()),\n",
        "                     Variable(torch.randn(x.size(0), x.size(1), x.size(2), x.size(3)).cuda()))\n",
        "      \n",
        "        it = torch.sigmoid(self.conv_i_xx(x)+self.conv_i_hh(state[0]))\n",
        "\n",
        "        ft = torch.sigmoid(self.conv_f_xx(x) + self.conv_f_hh(state[0]))\n",
        "        gt = torch.tanh(self.conv_c_xx(x) + self.conv_c_hh(state[0]))\n",
        "        ot = torch.sigmoid(self.conv_o_xx(x) + self.conv_o_hh(state[0]))\n",
        "\n",
        "        ct = torch.mul(ft, state[1]) + torch.mul(it,gt)\n",
        "        ht = torch.mul(ot,torch.tanh(ct))\n",
        "\n",
        "        return  ht, ct\n",
        "\n",
        "\n",
        "#Network \n",
        "class ourModel(nn.Module):\n",
        "    def __init__(self, num_classes=61, mem_size=512, homework_step = 0 , DEVICE=\"\"):\n",
        "        super(ourModel, self).__init__()\n",
        "        self.DEVICE = DEVICE\n",
        "        self.num_classes = num_classes\n",
        "        self.resNet = resnetMod.resnet34(True, True)\n",
        "        self.mem_size = mem_size\n",
        "        self.weight_softmax = self.resNet.fc.weight\n",
        "        self.homework_step = homework_step\n",
        "        if self.homework_step == 1:\n",
        "          self.lstm_cell = MyLSTMCell(512, mem_size)\n",
        "        elif self.homework_step == 2:\n",
        "          self.lstm_cell = MyConvLSTMCell(512, mem_size)\n",
        "\n",
        "        self.avgpool = nn.AvgPool2d(7)\n",
        "        self.dropout = nn.Dropout(0.7)\n",
        "        self.fc = nn.Linear(mem_size, self.num_classes)\n",
        "        self.classifier = nn.Sequential(self.dropout, self.fc)\n",
        "\n",
        "    def forward(self, inputVariable):\n",
        "        #Learning without Temporal information (mean)\n",
        "        if self.homework_step == 0:\n",
        "            video_level_features = torch.zeros((inputVariable.size(1), self.mem_size)).to(self.DEVICE)\n",
        "            for t in range(inputVariable.size(0)):\n",
        "                #spatial_frame_feat: (bs, 512, 7, 7)\n",
        "                _, spatial_frame_feat, _ = self.resNet(inputVariable[t])\n",
        "                #frames_feat: (bs, 512)\n",
        "                frame_feat = self.avgpool(spatial_frame_feat).view(spatial_frame_feat.size(0), -1)\n",
        "                video_level_features = video_level_features + frame_feat\n",
        "\n",
        "            video_level_features = video_level_features / inputVariable.size(0)\n",
        "            logits = self.classifier(video_level_features)\n",
        "            return logits, video_level_features\n",
        "\n",
        "        #Learning with Temporal information (LSTM)\n",
        "        elif self.homework_step == 1:\n",
        "            state = ( torch.zeros((inputVariable.size(1), self.mem_size)).to(self.DEVICE),\n",
        "                     torch.zeros((inputVariable.size(1), self.mem_size)).to(self.DEVICE) ) \n",
        "            for t in range(inputVariable.size(0)):\n",
        "                #spatial_frame_feat: (bs, 512, 7, 7)\n",
        "                _, spatial_frame_feat, _ = self.resNet(inputVariable[t])\n",
        "                #frames_feat: (bs, 512)\n",
        "                frame_feat = self.avgpool(spatial_frame_feat).view(state[1].size(0), -1)\n",
        "                state = self.lstm_cell(frame_feat, state)\n",
        "\n",
        "            video_level_features = state[1]\n",
        "            logits = self.classifier(video_level_features)\n",
        "            return logits, video_level_features\n",
        "\n",
        "        #Learning with Temporal information (ConvLSTM)\n",
        "        elif self.homework_step == 2:\n",
        "            state = (torch.zeros((inputVariable.size(1), self.mem_size, 7, 7)).to(self.DEVICE),\n",
        "                     torch.zeros((inputVariable.size(1), self.mem_size, 7, 7)).to(self.DEVICE))\n",
        "            for t in range(inputVariable.size(0)):\n",
        "                #spatial_frame_feat: (bs, 512, 7, 7)\n",
        "                _, spatial_frame_feat, _ = self.resNet(inputVariable[t])\n",
        "                state = self.lstm_cell(spatial_frame_feat, state)\n",
        "            video_level_features = self.avgpool(state[1]).view(state[1].size(0), -1)\n",
        "            logits = self.classifier(video_level_features)\n",
        "            return logits, video_level_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru8vllrMbgvL"
      },
      "source": [
        "#Build Model - Loss - Opt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZe-ZbEL7z3x"
      },
      "source": [
        "#CUDA_LAUNCH_BLOCKING=1\n",
        "validate = True\n",
        "\n",
        "model = ourModel(num_classes=NUM_CLASSES, mem_size=MEM_SIZE, homework_step=homework_step, DEVICE=DEVICE) #model\n",
        "\n",
        "#Train only the lstm cell and classifier\n",
        "model.train(False)\n",
        "for params in model.parameters():\n",
        "    params.requires_grad = False\n",
        "\n",
        "if homework_step > 0:\n",
        "    for params in model.lstm_cell.parameters():\n",
        "        params.requires_grad = True\n",
        "    model.lstm_cell.train(True)\n",
        "\n",
        "for params in model.classifier.parameters():\n",
        "    params.requires_grad = True\n",
        "model.classifier.train(True)\n",
        "\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "#model.load_state_dict(torch.load(\"/content/best_model_state_dict_rgb_split2.pth\", map_location=torch.device('cpu')), strict=True)\n",
        "\n",
        "\n",
        "#Loss\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "#Opt\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer_fn = optim.Adam(trainable_params, lr=LR, weight_decay=WEIGHT_DECAY, eps=1e-4)\n",
        "#Scheduler\n",
        "optim_scheduler = optim.lr_scheduler.MultiStepLR(optimizer_fn, milestones=STEP_SIZE, gamma=GAMMA)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0MWgLingzhw"
      },
      "source": [
        "#Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-uE2A9eHmtn"
      },
      "source": [
        "train_iter = 0\n",
        "val_iter = 0\n",
        "min_accuracy = 0\n",
        "\n",
        "trainSamples = len(train_dataset) - (len(train_dataset) % BATCH_SIZE)\n",
        "val_samples = len(test_dataset) \n",
        "iterPerEpoch = len(train_loader)\n",
        "val_steps = len(val_loader)\n",
        "cudnn.benchmark\n",
        "model_checkpoint = \"model\" #name\n",
        "\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    epoch_loss = 0\n",
        "    numCorrTrain = 0\n",
        "    \n",
        "    #blocks to train\n",
        "    if homework_step > 0:\n",
        "        model.lstm_cell.train(True)\n",
        "    model.classifier.train(True)\n",
        "    \n",
        "    \n",
        "    for i, (inputs, targets) in enumerate(train_loader):\n",
        "        train_iter += 1\n",
        "        optimizer_fn.zero_grad()\n",
        "        \n",
        "        # (BS, Frames, C, W, H) --> (Frames, BS, C, W, H)\n",
        "        inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
        "        labelVariable = targets.to(DEVICE)\n",
        "\n",
        "        # feeds in model\n",
        "        output_label, _ = model(inputVariable)\n",
        "        \n",
        "        # compute loss \n",
        "        loss = loss_fn(output_label, labelVariable)\n",
        "\n",
        "        # backward loss and optimizer step \n",
        "        loss.backward()\n",
        "        optimizer_fn.step()\n",
        "        \n",
        "        #compute the training accuracy \n",
        "        _, predicted = torch.max(output_label.data, 1)\n",
        "        numCorrTrain += torch.sum(predicted == labelVariable.data).data.item()\n",
        "        step_loss = loss.data.item()\n",
        "        epoch_loss += step_loss\n",
        "    \n",
        "    avg_loss = epoch_loss/iterPerEpoch\n",
        "    trainAccuracy = (numCorrTrain / trainSamples) * 100\n",
        "    #train_logger.add_epoch_data(epoch+1, trainAccuracy, avg_loss)\n",
        "    print(Fore.BLACK + 'Train: Epoch = {} | Loss = {:.3f} | Accuracy = {:.3f}'.format(epoch+1, avg_loss, trainAccuracy))\n",
        "    if validate:\n",
        "        if (epoch+1) % 1 == 0:\n",
        "            model.train(False)\n",
        "            val_loss_epoch = 0\n",
        "            numCorr = 0\n",
        "            for j, (inputs, targets) in enumerate(val_loader):\n",
        "                val_iter += 1\n",
        "                inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
        "                labelVariable = targets.to(DEVICE)\n",
        "                \n",
        "                output_label, _ = model(inputVariable)\n",
        "                val_loss = loss_fn(output_label, labelVariable)\n",
        "                val_loss_step = val_loss.data.item()\n",
        "                val_loss_epoch += val_loss_step\n",
        "                _, predicted = torch.max(output_label.data, 1)\n",
        "                numCorr += torch.sum(predicted == labelVariable.data).data.item()\n",
        "                #val_logger.add_step_data(val_iter, numCorr, val_loss_step)\n",
        "                \n",
        "            val_accuracy = (numCorr / val_samples) * 100\n",
        "            avg_val_loss = val_loss_epoch / val_steps\n",
        "\n",
        "            print(Fore.GREEN + 'Val: Epoch = {} | Loss {:.3f} | Accuracy = {:.3f}'.format(epoch + 1, avg_val_loss, val_accuracy))\n",
        "            if val_accuracy > min_accuracy:\n",
        "                print(\"[||| NEW BEST on val||||]\")\n",
        "                save_path_model = os.path.join(model_folder, model_checkpoint)\n",
        "                torch.save(model.state_dict(), save_path_model)\n",
        "                min_accuracy = val_accuracy\n",
        "    \n",
        "    optim_scheduler.step()\n",
        "\n",
        "print(Fore.CYAN + \"Best Acc --> \", min_accuracy)\n",
        "print(Fore.CYAN + \"Last Acc --> \", val_accuracy)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrLs_T2Qd0kc"
      },
      "source": [
        "#Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqK1ExB0cl8D"
      },
      "source": [
        "model.train(False)\n",
        "val_loss_epoch = 0\n",
        "numCorr = 0\n",
        "val_iter = 0\n",
        "val_samples = len(test_dataset) \n",
        "val_steps = len(val_loader)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for j, (inputs, targets) in enumerate(val_loader):\n",
        "        val_iter += 1\n",
        "        inputVariable = inputs.permute(1, 0, 2, 3, 4).to(DEVICE)\n",
        "        labelVariable = targets.to(DEVICE)\n",
        "        \n",
        "        output_label, _ = model(inputVariable)\n",
        "        val_loss = loss_fn(output_label, labelVariable)\n",
        "        val_loss_step = val_loss.data.item()\n",
        "        val_loss_epoch += val_loss_step\n",
        "        _, predicted = torch.max(output_label.data, 1)\n",
        "        numCorr += torch.sum(predicted == labelVariable.data).data.item()\n",
        "        \n",
        "    val_accuracy = (numCorr / val_samples) * 100\n",
        "    avg_val_loss = val_loss_epoch / val_steps\n",
        "\n",
        "print('Loss {:.3f} | Accuracy = {:.3f}'.format(avg_val_loss, val_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gr9BxL8zeBfv"
      },
      "source": [
        "#**Learning with Temporal information** (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-qHYgnyf_wn"
      },
      "source": [
        "#**Learning with Spatio-Temporal information** (ConvLSTM)\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}